
xconfig=`mktemp`
cat << EOF > $xconfig

input dim=$ivec_dim name=ivector
input dim=$feat_dim name=input

# please note that it is important to have input layer with the name=input
# as the layer immediately preceding the fixed-affine-layer to enable
# the use of short notation for the descriptor
fixed-affine-layer name=lda input=Append(-1,0,1,ReplaceIndex(ivector, t, 0)) affine-transform-file=$dir/configs/lda.mat

# the first splicing is moved before the lda layer, so no splicing here
relu-renorm-layer name=tdnn1 dim=$relu_dim
relu-renorm-layer name=tdnn2 input=Append(-1,0,1) dim=$relu_dim
relu-renorm-layer name=tdnn3 input=Append(-1,0,1) dim=$relu_dim

# check steps/libs/nnet3/xconfig/lstm.py for the other options and defaults
lstmp-layer name=lstm1 cell-dim=$(($lstm_dim * 4)) recurrent-projection-dim=$lstm_dim non-recurrent-projection-dim=$lstm_dim delay=-3
relu-renorm-layer name=tdnn4 input=Append(0,3) dim=$relu_dim
relu-renorm-layer name=tdnn5 input=Append(0,3) dim=$relu_dim
relu-renorm-layer name=tdnn6 input=Append(0,3) dim=$relu_dim
lstmp-layer name=lstm2 cell-dim=$(($lstm_dim * 4)) recurrent-projection-dim=$lstm_dim non-recurrent-projection-dim=256 delay=-3
relu-renorm-layer name=tdnn7 input=Append(0,3) dim=$relu_dim
relu-renorm-layer name=tdnn8 input=Append(0,3) dim=$relu_dim
relu-renorm-layer name=tdnn9 input=Append(0,3) dim=$relu_dim
lstmp-layer name=lstm3 cell-dim=$(($lstm_dim * 4)) recurrent-projection-dim=$lstm_dim non-recurrent-projection-dim=256 delay=-3

## adding the layers for chain branch
output-layer name=output input=lstm3 output-delay=$label_delay include-log-softmax=false dim=$num_targets max-change=1.5

# adding the layers for xent branch
# This block prints the configs for a separate output that will be
# trained with a cross-entropy objective in the 'chain' models... this
# has the effect of regularizing the hidden parts of the model.  we use
# 0.5 / args.xent_regularize as the learning rate factor- the factor of
# 0.5 / args.xent_regularize is suitable as it means the xent
# final-layer learns at a rate independent of the regularization
# constant; and the 0.5 was tuned so as to make the relative progress
# similar in the xent and regular final layers.
output-layer name=output-xent input=lstm3 output-delay=$label_delay dim=$num_targets learning-rate-factor=$learning_rate_factor max-change=1.5

EOF
